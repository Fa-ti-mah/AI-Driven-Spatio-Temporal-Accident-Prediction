{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71750fd8-80b5-4501-a13c-c0d13875daa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLASS DISTRIBUTION IN y_train =====\n",
      "Severity 1 (internal 0): 53,891\n",
      "Severity 2 (internal 1): 4,347,102\n",
      "Severity 3 (internal 2): 1,039,406\n",
      "Severity 4 (internal 3): 147,783\n"
     ]
    }
   ],
   "source": [
    "# Print class distribution\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "print(\"\\n===== CLASS DISTRIBUTION IN y_train =====\")\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f\"Severity {cls+1} (internal {cls}): {cnt:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be0aee-de68-488f-8e69-ab40d03c532c",
   "metadata": {},
   "source": [
    "FIRST COMPLETED SEQUENCE- STRONG ACCURACY, NOT CAPTURING SEVERITY 1 AND 4 PROPERLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa137bff-0cea-4f97-bb2a-b81ee2e2b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLASS DISTRIBUTION IN y_train =====\n",
      "Severity 1 (internal 0): 53,891\n",
      "Severity 2 (internal 1): 4,347,102\n",
      "Severity 3 (internal 2): 1,039,406\n",
      "Severity 4 (internal 3): 147,783\n",
      "\n",
      "===== XGBOOST (MEMORY OPTIMIZED, FIXED SEVERITY) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.29      0.05      0.09     13473\n",
      "           2       0.84      0.94      0.89   1086776\n",
      "           3       0.64      0.41      0.50    259851\n",
      "           4       0.33      0.08      0.13     36946\n",
      "\n",
      "    accuracy                           0.81   1397046\n",
      "   macro avg       0.52      0.37      0.40   1397046\n",
      "weighted avg       0.78      0.81      0.79   1397046\n",
      "\n",
      "Accuracy: 0.8120197903290228\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ======================================\n",
    "# 1. LOAD DATA\n",
    "# ======================================\n",
    "df = pd.read_parquet(\"US_Accidents_March23.parquet\")\n",
    "\n",
    "# Keep only memory-efficient severity predictors\n",
    "cols = [\n",
    "    'Severity','Distance(mi)','Temperature(F)','Visibility(mi)','Humidity(%)','Pressure(in)',\n",
    "    'Wind_Speed(mph)','Weather_Condition','Traffic_Signal','Junction','Crossing','Stop',\n",
    "    'Start_Time','State','City'\n",
    "]\n",
    "df = df[cols]\n",
    "\n",
    "# ======================================\n",
    "# 2. DATETIME → NUMERIC\n",
    "# ======================================\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df.dropna(subset=['Start_Time'], inplace=True)\n",
    "df['Hour'] = df['Start_Time'].dt.hour.astype('int8')\n",
    "df['Month'] = df['Start_Time'].dt.month.astype('int8')\n",
    "df['Weekday'] = df['Start_Time'].dt.weekday.astype('int8')\n",
    "df.drop(columns=['Start_Time'], inplace=True)\n",
    "\n",
    "# ======================================\n",
    "# 3. WEATHER SIMPLIFICATION\n",
    "# ======================================\n",
    "df['Weather_Condition'] = df['Weather_Condition'].fillna('')\n",
    "df['Rain'] = df['Weather_Condition'].str.contains('Rain|Storm|Thunder', case=False, regex=True).astype('int8')\n",
    "df['Snow'] = df['Weather_Condition'].str.contains('Snow|Ice|Blizzard', case=False, regex=True).astype('int8')\n",
    "df['Fog']  = df['Weather_Condition'].str.contains('Fog', case=False, regex=True).astype('int8')\n",
    "df.drop(columns=['Weather_Condition'], inplace=True)\n",
    "\n",
    "# ======================================\n",
    "# 4. LOCATION TARGET ENCODING\n",
    "# ======================================\n",
    "te = TargetEncoder(cols=['State', 'City'])\n",
    "df[['State','City']] = te.fit_transform(df[['State','City']], df['Severity'])\n",
    "\n",
    "# ======================================\n",
    "# 5. BINARY INFRA FEATURES\n",
    "# ======================================\n",
    "for col in ['Traffic_Signal','Junction','Crossing','Stop']:\n",
    "    df[col] = df[col].astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 6. REDUCE NUMERIC PRECISION\n",
    "# ======================================\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "df[num_cols] = df[num_cols].astype('float32')\n",
    "\n",
    "# ======================================\n",
    "# 7. TARGET FIX: Severity 1–4 → 0–3\n",
    "# ======================================\n",
    "df['Severity_Internal'] = df['Severity'] - 1\n",
    "df.drop(columns=['Severity'], inplace=True)\n",
    "\n",
    "X = df.drop(['Severity_Internal'], axis=1)\n",
    "y = df['Severity_Internal'].astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 8. IMPUTATION FOR NUMERIC & CATEGORICAL\n",
    "# ======================================\n",
    "num_cols = X.select_dtypes(include=['int64','float64','float32']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "X = X.astype('float32')  # ensure numeric type for SMOTE/XGBoost\n",
    "\n",
    "# ======================================\n",
    "# 9. TRAIN-TEST SPLIT\n",
    "# ======================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Print class distribution for reference\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\n===== CLASS DISTRIBUTION IN y_train =====\")\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f\"Severity {cls+1} (internal {cls}): {cnt:,}\")\n",
    "\n",
    "# ======================================\n",
    "# 10. APPLY SMOTE ON TRUE MINORITY CLASSES\n",
    "# ======================================\n",
    "sm = SMOTE(\n",
    "    sampling_strategy={\n",
    "        0: 200_000,   # Severity 1\n",
    "        3: 400_000    # Severity 4\n",
    "    },\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# ======================================\n",
    "# 11. MEMORY-OPTIMIZED XGBOOST\n",
    "# ======================================\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    tree_method='hist',\n",
    "    max_depth=5,\n",
    "    n_estimators=350,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ======================================\n",
    "# 12. PREDICT & MAP BACK 0–3 → 1–4\n",
    "# ======================================\n",
    "y_pred_internal = xgb.predict(X_test)\n",
    "y_pred = y_pred_internal + 1\n",
    "y_true = y_test + 1\n",
    "\n",
    "print(\"\\n===== XGBOOST (MEMORY OPTIMIZED, FIXED SEVERITY) =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97fdca9-faad-45f5-b705-878ffb81cd88",
   "metadata": {},
   "source": [
    "SECOND SEQUENCE, WITH IMPROVED CLASS WEIGHTS, STILL NOT BALANCING, POOR ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c33c4df-3598-4250-a35c-4203c8e6efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBOOST (WITH CLASS WEIGHTS & SMOTE) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.04      0.88      0.07     13473\n",
      "           2       0.95      0.38      0.54   1086776\n",
      "           3       0.48      0.54      0.51    259851\n",
      "           4       0.09      0.84      0.16     36946\n",
      "\n",
      "    accuracy                           0.42   1397046\n",
      "   macro avg       0.39      0.66      0.32   1397046\n",
      "weighted avg       0.83      0.42      0.52   1397046\n",
      "\n",
      "Accuracy: 0.42218867524762965\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ======================================\n",
    "# 1. LOAD DATA\n",
    "# ======================================\n",
    "df = pd.read_parquet(\"US_Accidents_March23.parquet\")\n",
    "\n",
    "cols = [\n",
    "    'Severity','Distance(mi)','Temperature(F)','Visibility(mi)','Humidity(%)','Pressure(in)',\n",
    "    'Wind_Speed(mph)','Weather_Condition','Traffic_Signal','Junction','Crossing','Stop',\n",
    "    'Start_Time','State','City'\n",
    "]\n",
    "df = df[cols]\n",
    "\n",
    "# ======================================\n",
    "# 2. DATETIME → NUMERIC\n",
    "# ======================================\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df.dropna(subset=['Start_Time'], inplace=True)\n",
    "df['Hour'] = df['Start_Time'].dt.hour.astype('int8')\n",
    "df['Month'] = df['Start_Time'].dt.month.astype('int8')\n",
    "df['Weekday'] = df['Start_Time'].dt.weekday.astype('int8')\n",
    "df.drop(columns=['Start_Time'], inplace=True)\n",
    "\n",
    "# ======================================\n",
    "# 3. WEATHER SIMPLIFICATION\n",
    "# ======================================\n",
    "df['Weather_Condition'] = df['Weather_Condition'].fillna('')\n",
    "df['Rain'] = df['Weather_Condition'].str.contains('Rain|Storm|Thunder', case=False, regex=True).astype('int8')\n",
    "df['Snow'] = df['Weather_Condition'].str.contains('Snow|Ice|Blizzard', case=False, regex=True).astype('int8')\n",
    "df['Fog']  = df['Weather_Condition'].str.contains('Fog', case=False, regex=True).astype('int8')\n",
    "df.drop(columns=['Weather_Condition'], inplace=True)\n",
    "\n",
    "# ======================================\n",
    "# 4. LOCATION TARGET ENCODING\n",
    "# ======================================\n",
    "te = TargetEncoder(cols=['State','City'])\n",
    "df[['State','City']] = te.fit_transform(df[['State','City']], df['Severity'])\n",
    "\n",
    "# ======================================\n",
    "# 5. BINARY INFRA FEATURES\n",
    "# ======================================\n",
    "for col in ['Traffic_Signal','Junction','Crossing','Stop']:\n",
    "    df[col] = df[col].astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 6. REDUCE NUMERIC PRECISION\n",
    "# ======================================\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "df[num_cols] = df[num_cols].astype('float32')\n",
    "\n",
    "# ======================================\n",
    "# 7. TARGET FIX: Severity 1–4 → 0–3\n",
    "# ======================================\n",
    "df['Severity_Internal'] = df['Severity'] - 1\n",
    "df.drop(columns=['Severity'], inplace=True)\n",
    "\n",
    "X = df.drop(['Severity_Internal'], axis=1)\n",
    "y = df['Severity_Internal'].astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 8. IMPUTATION FOR NUMERIC & CATEGORICAL\n",
    "# ======================================\n",
    "num_cols = X.select_dtypes(include=['int64','float64','float32']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "X = X.astype('float32')  # ensure numeric type\n",
    "\n",
    "# ======================================\n",
    "# 9. TRAIN-TEST SPLIT\n",
    "# ======================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# 10. CALCULATE CLASS WEIGHTS\n",
    "# ======================================\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = {cls: weight for cls, weight in zip(classes, class_weights)}\n",
    "\n",
    "# Convert to array of weights for XGBoost\n",
    "sample_weights = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "\n",
    "# ======================================\n",
    "# 11. APPLY SMOTE ON TRUE MINORITY CLASSES\n",
    "# ======================================\n",
    "sm = SMOTE(\n",
    "    sampling_strategy={\n",
    "        0: 150_000,  # Severity 1\n",
    "        3: 200_000   # Severity 4\n",
    "    },\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "# Update sample weights after resampling\n",
    "sample_weights = np.array([class_weight_dict[cls] for cls in y_train])\n",
    "\n",
    "# ======================================\n",
    "# 12. MEMORY-OPTIMIZED XGBOOST WITH CLASS WEIGHTS\n",
    "# ======================================\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    tree_method='hist',\n",
    "    max_depth=5,\n",
    "    n_estimators=350,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# ======================================\n",
    "# 13. PREDICT & MAP BACK 0–3 → 1–4\n",
    "# ======================================\n",
    "y_pred_internal = xgb.predict(X_test)\n",
    "y_pred = y_pred_internal + 1\n",
    "y_true = y_test + 1\n",
    "\n",
    "print(\"\\n===== XGBOOST (WITH CLASS WEIGHTS & SMOTE) =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f390c8b-b317-4332-8310-c93c0aae8e24",
   "metadata": {},
   "source": [
    "IMPROVED WEIGHTS AND BALANCING, OVER SAMPLED SEVERITY 1 AND 4, IGNORED 2 AND 3, POOR ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c998aa-81c7-44be-b085-ca8677e9ec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:33:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== XGBOOST (CLASS WEIGHTS ONLY) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.05      0.81      0.09     13473\n",
      "           2       0.95      0.44      0.60   1086776\n",
      "           3       0.46      0.63      0.53    259851\n",
      "           4       0.10      0.82      0.17     36946\n",
      "\n",
      "    accuracy                           0.49   1397046\n",
      "   macro avg       0.39      0.67      0.35   1397046\n",
      "weighted avg       0.82      0.49      0.57   1397046\n",
      "\n",
      "Accuracy: 0.4901492148433194\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# --------------------------------------\n",
    "# 1. LOAD DATA\n",
    "# --------------------------------------\n",
    "df = pd.read_parquet(\"US_Accidents_March23.parquet\")\n",
    "\n",
    "# Keep only relevant columns for memory efficiency\n",
    "cols = [\n",
    "    'Severity','Distance(mi)','Temperature(F)','Visibility(mi)','Humidity(%)','Pressure(in)',\n",
    "    'Wind_Speed(mph)','Weather_Condition','Traffic_Signal','Junction','Crossing','Stop',\n",
    "    'Start_Time','State','City'\n",
    "]\n",
    "df = df[cols]\n",
    "\n",
    "# --------------------------------------\n",
    "# 2. DATETIME → NUMERIC\n",
    "# --------------------------------------\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df.dropna(subset=['Start_Time'], inplace=True)\n",
    "df['Hour'] = df['Start_Time'].dt.hour.astype('int8')\n",
    "df['Month'] = df['Start_Time'].dt.month.astype('int8')\n",
    "df['Weekday'] = df['Start_Time'].dt.weekday.astype('int8')\n",
    "df.drop(columns=['Start_Time'], inplace=True)\n",
    "\n",
    "# --------------------------------------\n",
    "# 3. SIMPLIFY WEATHER\n",
    "# --------------------------------------\n",
    "df['Weather_Condition'] = df['Weather_Condition'].fillna('')\n",
    "df['Rain'] = df['Weather_Condition'].str.contains('Rain|Storm|Thunder', case=False, regex=True).astype('int8')\n",
    "df['Snow'] = df['Weather_Condition'].str.contains('Snow|Ice|Blizzard', case=False, regex=True).astype('int8')\n",
    "df['Fog']  = df['Weather_Condition'].str.contains('Fog', case=False, regex=True).astype('int8')\n",
    "df.drop(columns=['Weather_Condition'], inplace=True)\n",
    "\n",
    "# --------------------------------------\n",
    "# 4. LOCATION TARGET ENCODING\n",
    "# --------------------------------------\n",
    "te = TargetEncoder(cols=['State', 'City'])\n",
    "df[['State','City']] = te.fit_transform(df[['State','City']], df['Severity'])\n",
    "\n",
    "# --------------------------------------\n",
    "# 5. BINARY INFRA FEATURES\n",
    "# --------------------------------------\n",
    "for col in ['Traffic_Signal','Junction','Crossing','Stop']:\n",
    "    df[col] = df[col].astype('int8')\n",
    "\n",
    "# --------------------------------------\n",
    "# 6. REDUCE NUMERIC PRECISION\n",
    "# --------------------------------------\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "df[num_cols] = df[num_cols].astype('float32')\n",
    "\n",
    "# --------------------------------------\n",
    "# 7. TARGET FIX: Convert Severity 1–4 → 0–3\n",
    "# --------------------------------------\n",
    "df['Severity_Internal'] = df['Severity'] - 1\n",
    "df.drop(columns=['Severity'], inplace=True)\n",
    "\n",
    "X = df.drop(['Severity_Internal'], axis=1)\n",
    "y = df['Severity_Internal'].astype('int8')\n",
    "\n",
    "# --------------------------------------\n",
    "# 8. IMPUTE MISSING VALUES\n",
    "# --------------------------------------\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "num_cols = X.select_dtypes(include=['int64','float32']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "X = X.astype('float32')  # ensure float32 for XGBoost\n",
    "\n",
    "# --------------------------------------\n",
    "# 9. TRAIN-TEST SPLIT\n",
    "# --------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --------------------------------------\n",
    "# 10. COMPUTE CLASS WEIGHTS\n",
    "# --------------------------------------\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "# --------------------------------------\n",
    "# 11. MEMORY-OPTIMIZED XGBOOST\n",
    "# --------------------------------------\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    tree_method='hist',\n",
    "    max_depth=5,\n",
    "    n_estimators=350,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# --------------------------------------\n",
    "# 12. PREDICT & MAP BACK 0–3 → 1–4\n",
    "# --------------------------------------\n",
    "y_pred_internal = xgb.predict(X_test)\n",
    "y_pred = y_pred_internal + 1\n",
    "y_true = y_test + 1\n",
    "\n",
    "print(\"===== XGBOOST (CLASS WEIGHTS ONLY) =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc686d2e-9195-46e3-9e60-04b6fcaa2adc",
   "metadata": {},
   "source": [
    "WITH ADDITIONAL TEMPORAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20052fce-2c3a-4094-96e1-39abc81e7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\Temp\\ipykernel_1988\\2759865492.py:71: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df['Highway'] = df['City'].astype(str).str.contains(r'\\b(I-|US-|SR-|Hwy)', case=False, regex=True).astype('int8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CLASS DISTRIBUTION IN y_train =====\n",
      "Severity 1: 53,891\n",
      "Severity 2: 4,347,102\n",
      "Severity 3: 1,039,406\n",
      "Severity 4: 147,783\n",
      "\n",
      "===== XGBOOST WITH ENHANCED FEATURES =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.33      0.05      0.09     13473\n",
      "           2       0.84      0.94      0.89   1086776\n",
      "           3       0.65      0.42      0.51    259851\n",
      "           4       0.42      0.13      0.19     36946\n",
      "\n",
      "    accuracy                           0.82   1397046\n",
      "   macro avg       0.56      0.39      0.42   1397046\n",
      "weighted avg       0.79      0.82      0.79   1397046\n",
      "\n",
      "Accuracy: 0.8162580187051822\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ======================================\n",
    "# 1. LOAD DATA\n",
    "# ======================================\n",
    "df = pd.read_parquet(\"US_Accidents_March23.parquet\")\n",
    "\n",
    "cols = [\n",
    "    'Severity','Distance(mi)','Temperature(F)','Visibility(mi)','Humidity(%)','Pressure(in)',\n",
    "    'Wind_Speed(mph)','Weather_Condition','Traffic_Signal','Junction','Crossing','Stop',\n",
    "    'Start_Time','State','City'\n",
    "]\n",
    "df = df[cols]\n",
    "\n",
    "# ======================================\n",
    "# 2. DATETIME → NUMERIC\n",
    "# ======================================\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "df.dropna(subset=['Start_Time'], inplace=True)\n",
    "\n",
    "df['Hour'] = df['Start_Time'].dt.hour.astype('int8')\n",
    "df['Month'] = df['Start_Time'].dt.month.astype('int8')\n",
    "df['Weekday'] = df['Start_Time'].dt.weekday.astype('int8')\n",
    "df['Weekend'] = (df['Weekday'] >= 5).astype('int8')\n",
    "df['Night'] = (df['Hour'] >= 19).astype('int8')\n",
    "df['Is_Rush_Hour'] = df['Hour'].isin([7,8,17,18]).astype('int8')\n",
    "\n",
    "df.drop(columns=['Start_Time'], inplace=True)\n",
    "\n",
    "# ======================================\n",
    "# 3. WEATHER SIMPLIFICATION\n",
    "# ======================================\n",
    "df['Weather_Condition'] = df['Weather_Condition'].fillna('')\n",
    "df['Rain'] = df['Weather_Condition'].str.contains('Rain|Storm|Thunder', case=False).astype('int8')\n",
    "df['Snow'] = df['Weather_Condition'].str.contains('Snow|Ice|Blizzard', case=False).astype('int8')\n",
    "df['Fog']  = df['Weather_Condition'].str.contains('Fog', case=False).astype('int8')\n",
    "\n",
    "# NEW DERIVED WEATHER SIGNALS\n",
    "df['Low_Visibility'] = (df['Visibility(mi)'] < 1).astype('int8')\n",
    "df['High_Wind'] = (df['Wind_Speed(mph)'] > 30).astype('int8')\n",
    "df['Temp_Below_Freezing'] = (df['Temperature(F)'] < 32).astype('int8')\n",
    "\n",
    "df.drop(columns=['Weather_Condition'], inplace=True)\n",
    "\n",
    "# ======================================\n",
    "# 4. LOCATION TARGET ENCODING\n",
    "# ======================================\n",
    "te = TargetEncoder(cols=['State', 'City'])\n",
    "df[['State','City']] = te.fit_transform(df[['State','City']], df['Severity'])\n",
    "\n",
    "# ADD CITY DENSITY SIGNAL\n",
    "city_counts = df['City'].value_counts().to_dict()\n",
    "df['City_Frequency'] = df['City'].map(city_counts).astype('float32')\n",
    "\n",
    "# ======================================\n",
    "# 5. ROAD / INFRA FEATURES\n",
    "# ======================================\n",
    "for col in ['Traffic_Signal','Junction','Crossing','Stop']:\n",
    "    df[col] = df[col].astype('int8')\n",
    "\n",
    "# PROXY FOR HIGHWAY SPEED ROADS\n",
    "df['Highway'] = df['City'].astype(str).str.contains(r'\\b(I-|US-|SR-|Hwy)', case=False, regex=True).astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 6. DISTANCE NONLINEARITY\n",
    "# ======================================\n",
    "df['Short_Distance'] = (df['Distance(mi)'] < 0.2).astype('int8')\n",
    "df['Long_Distance']  = (df['Distance(mi)'] > 2).astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 7. REDUCE NUMERIC PRECISION\n",
    "# ======================================\n",
    "num_cols = df.select_dtypes(include=['float64','int64']).columns\n",
    "df[num_cols] = df[num_cols].astype('float32')\n",
    "\n",
    "# ======================================\n",
    "# 8. TARGET FIX: Severity 1–4 → 0–3\n",
    "# ======================================\n",
    "df['Severity_Internal'] = df['Severity'] - 1\n",
    "df.drop(columns=['Severity'], inplace=True)\n",
    "\n",
    "X = df.drop(['Severity_Internal'], axis=1)\n",
    "y = df['Severity_Internal'].astype('int8')\n",
    "\n",
    "# ======================================\n",
    "# 9. IMPUTATION\n",
    "# ======================================\n",
    "num_cols = X.select_dtypes(include=['float32','float64','int64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='median'), num_cols),\n",
    "        ('cat', SimpleImputer(strategy='most_frequent'), cat_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "X = preprocessor.fit_transform(X).astype('float32')\n",
    "\n",
    "# ======================================\n",
    "# 10. TRAIN-TEST SPLIT\n",
    "# ======================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(\"\\n===== CLASS DISTRIBUTION IN y_train =====\")\n",
    "for cls, cnt in zip(unique, counts):\n",
    "    print(f\"Severity {cls+1}: {cnt:,}\")\n",
    "\n",
    "# ======================================\n",
    "# 11. SMOTE\n",
    "# ======================================\n",
    "sm = SMOTE(\n",
    "    sampling_strategy={0: 200_000, 3: 400_000},\n",
    "    random_state=42\n",
    ")\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# ======================================\n",
    "# 12. XGBOOST\n",
    "# ======================================\n",
    "xgb = XGBClassifier(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    tree_method='hist',\n",
    "    max_depth=6,                  # ← Adjusted for new features\n",
    "    n_estimators=400,             # ← More trees\n",
    "    learning_rate=0.045,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# ======================================\n",
    "# 13. PREDICT & MAP BACK\n",
    "# ======================================\n",
    "y_pred_internal = xgb.predict(X_test)\n",
    "y_pred = y_pred_internal + 1\n",
    "y_true = y_test + 1\n",
    "\n",
    "print(\"\\n===== XGBOOST WITH ENHANCED FEATURES =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b960661-4e88-4aca-a7fe-d7a650f15342",
   "metadata": {},
   "source": [
    "IMPLEMENTING CLASS WEIGHTS + FOCAL LOSS TO ADDED FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a591263-e963-4f41-9c1f-603dd9f65d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 7.483135, 1: 0.3442815466487789, 2: 1.4398868199721764, 3: 3.7415675}\n",
      "\n",
      "--- Focal reweighting iteration 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:16:42] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg p_t by class (train): {0: 0.6731884479522705, 1: 0.46296611428260803, 2: 0.5538685917854309, 3: 0.6795997619628906}\n",
      "Avg focal factor by class (train): {0: 0.1793413907289505, 1: 0.3437821567058563, 2: 0.2633805274963379, 3: 0.16095396876335144}\n",
      "\n",
      "--- Focal reweighting iteration 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:25:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg p_t by class (train): {0: 0.3731985092163086, 1: 0.3801787495613098, 2: 0.3061244487762451, 3: 0.30045434832572937}\n",
      "Avg focal factor by class (train): {0: 0.424507737159729, 1: 0.39740464091300964, 2: 0.49012163281440735, 3: 0.50416100025177}\n",
      "\n",
      "--- Focal reweighting iteration 3/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:34:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg p_t by class (train): {0: 0.6766406893730164, 1: 0.4387747645378113, 2: 0.5851922035217285, 3: 0.7054749131202698}\n",
      "Avg focal factor by class (train): {0: 0.18709827959537506, 1: 0.39136460423469543, 2: 0.2563002109527588, 3: 0.1570015847682953}\n",
      "\n",
      "===== XGBOOST with CLASS WEIGHTS + FOCAL REWEIGHTING =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.05      0.72      0.10     13473\n",
      "           2       0.94      0.49      0.64   1086776\n",
      "           3       0.45      0.69      0.54    259851\n",
      "           4       0.11      0.74      0.19     36946\n",
      "\n",
      "    accuracy                           0.53   1397046\n",
      "   macro avg       0.39      0.66      0.37   1397046\n",
      "weighted avg       0.82      0.53      0.61   1397046\n",
      "\n",
      "Accuracy: 0.5336953829723574\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# ITERATIVE FOCAL RE-WEIGHTING + CLASS WEIGHTS for XGBoost\n",
    "# ---------------------------\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "gamma = 2.0              # focal focusing parameter (common: 1.0-3.0)\n",
    "n_rounds = 3             # number of re-weighting iterations (2-3 is usually enough)\n",
    "xgb_params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": 4,\n",
    "    \"eta\": 0.08,\n",
    "    \"max_depth\": 8,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"gamma\": 0.1,\n",
    "    \"lambda\": 1.5,\n",
    "    \"tree_method\": \"hist\", # memory efficient\n",
    "}\n",
    "\n",
    "# Ensure arrays (X_train from preprocessor may already be np.array)\n",
    "X_train_arr = np.asarray(X_train, dtype=np.float32)\n",
    "y_train_arr = np.asarray(y_train, dtype=np.int32)\n",
    "\n",
    "# 1) compute class-level weights (balanced)\n",
    "classes = np.unique(y_train_arr)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_arr)\n",
    "# map into dict: class -> weight\n",
    "cw = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "print(\"Class weights:\", cw)\n",
    "\n",
    "# initialize sample weights as class weights\n",
    "sample_weight = np.array([cw[int(lbl)] for lbl in y_train_arr], dtype=np.float32)\n",
    "\n",
    "# iterative reweighting\n",
    "for it in range(n_rounds):\n",
    "    print(f\"\\n--- Focal reweighting iteration {it+1}/{n_rounds} ---\")\n",
    "    # train xgboost with current sample weights\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    model.fit(X_train_arr, y_train_arr, sample_weight=sample_weight, verbose=False)\n",
    "\n",
    "    # predict probabilities on training set (softprob)\n",
    "    proba = model.predict_proba(X_train_arr)  # shape (n_samples, n_classes)\n",
    "    # probability for the true class p_t\n",
    "    p_t = proba[np.arange(len(y_train_arr)), y_train_arr]\n",
    "\n",
    "    # compute focal reweighting factor: (1 - p_t) ** gamma\n",
    "    focal_factor = np.power(1.0 - np.clip(p_t, 1e-12, 1.0), gamma).astype(np.float32)\n",
    "\n",
    "    # new sample weight = class_weight * focal_factor\n",
    "    new_sample_weight = np.array([cw[int(lbl)] for lbl in y_train_arr], dtype=np.float32) * focal_factor\n",
    "\n",
    "    # stabilize weights by scaling to have same sum as before (keeps total weight scale stable)\n",
    "    if new_sample_weight.sum() > 0:\n",
    "        new_sample_weight = new_sample_weight * (sample_weight.sum() / new_sample_weight.sum())\n",
    "\n",
    "    sample_weight = new_sample_weight\n",
    "\n",
    "    # report training-side diagnostics (class-wise average p_t and focal factor)\n",
    "    avg_pt_by_class = {}\n",
    "    avg_factor_by_class = {}\n",
    "    for c in classes:\n",
    "        mask = (y_train_arr == c)\n",
    "        if mask.sum() == 0:\n",
    "            avg_pt_by_class[c] = np.nan\n",
    "            avg_factor_by_class[c] = np.nan\n",
    "        else:\n",
    "            avg_pt_by_class[c] = float(p_t[mask].mean())\n",
    "            avg_factor_by_class[c] = float(focal_factor[mask].mean())\n",
    "    print(\"Avg p_t by class (train):\", avg_pt_by_class)\n",
    "    print(\"Avg focal factor by class (train):\", avg_factor_by_class)\n",
    "\n",
    "# After iterations, final model is `model`\n",
    "# Evaluate on test set (X_test may be np.array; ensure dtype)\n",
    "X_test_arr = np.asarray(X_test, dtype=np.float32)\n",
    "y_test_arr = np.asarray(y_test, dtype=np.int32)\n",
    "\n",
    "y_pred_internal = model.predict(X_test_arr)\n",
    "# map back to 1-4 externally if you want\n",
    "y_pred = y_pred_internal + 1\n",
    "y_true = y_test_arr + 1\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(\"\\n===== XGBOOST with CLASS WEIGHTS + FOCAL REWEIGHTING =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aecd483-a644-4326-ada5-ac2a68405db1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3cccdc-2cec-4fe5-b802-4bca4240de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (5986508, 9) (1397046, 9) (5986508,)\n",
      "Baseline class weights: {0: 7.483135, 1: 0.3442815466487789, 2: 1.4398868199721764, 3: 3.7415675}\n",
      "Scaled class weights: {0: 3.0, 1: 0.13802298634814644, 2: 0.5772527770669017, 3: 1.5}\n",
      "\n",
      "--- Iteration 1/2 — training with current sample weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:47:59] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg p_t by class (train): {0: 0.6792340874671936, 1: 0.4683409333229065, 2: 0.5571920871734619, 3: 0.6835695505142212}\n",
      "Avg focal factor by class (train): {0: 0.5087133049964905, 1: 0.7066897749900818, 2: 0.6350737810134888, 3: 0.5160141587257385}\n",
      "\n",
      "--- Iteration 2/2 — training with current sample weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:56:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg p_t by class (train): {0: 0.6199984550476074, 1: 0.4510178565979004, 2: 0.5051166415214539, 3: 0.6096357107162476}\n",
      "Avg focal factor by class (train): {0: 0.5823259949684143, 1: 0.7306800484657288, 2: 0.6906033754348755, 3: 0.599987804889679}\n",
      "Using 50000 samples to calibrate probabilities (sigmoid).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 118\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X_cal)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples to calibrate probabilities (sigmoid).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Wrap the prefit model with CalibratedClassifierCV (cv='prefit') and fit on small calibration set\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m calibrated \u001b[38;5;241m=\u001b[39m CalibratedClassifierCV(base_estimator\u001b[38;5;241m=\u001b[39mfinal_model, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprefit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    119\u001b[0m calibrated\u001b[38;5;241m.\u001b[39mfit(X_cal, y_cal)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: CalibratedClassifierCV.__init__() got an unexpected keyword argument 'base_estimator'"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# FULL: Stable focal reweighting + calibration\n",
    "# =========================\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import gc\n",
    "\n",
    "# --- Parameters (tweakable) ---\n",
    "gamma = 0.5            # focal focusing parameter (reduced to avoid over-focus)\n",
    "n_rounds = 2           # iterative reweighting rounds (2 is usually enough)\n",
    "max_calib_samples = 50000  # max samples to use for probability calibration\n",
    "random_state = 42\n",
    "\n",
    "# --- Ensure numpy arrays and dtypes (memory friendly) ---\n",
    "X_train_arr = np.asarray(X_train, dtype=np.float32)\n",
    "X_test_arr = np.asarray(X_test, dtype=np.float32)\n",
    "y_train_arr = np.asarray(y_train, dtype=np.int32)  # internal labels 0..3\n",
    "y_test_arr  = np.asarray(y_test, dtype=np.int32)\n",
    "\n",
    "print(\"Shapes:\", X_train_arr.shape, X_test_arr.shape, y_train_arr.shape)\n",
    "\n",
    "# --- Compute class-level balanced weights (as baseline) ---\n",
    "classes = np.unique(y_train_arr)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_arr)\n",
    "cw_dict = {int(c): float(w) for c, w in zip(classes, cw)}\n",
    "print(\"Baseline class weights:\", cw_dict)\n",
    "\n",
    "# Scale down the class weights a bit to avoid huge gradients (optional)\n",
    "# e.g., bring max weight to at most 3.0\n",
    "max_allowed = 3.0\n",
    "scale = min(1.0, max_allowed / max(cw_dict.values()))\n",
    "if scale < 1.0:\n",
    "    cw_dict = {k: float(v * scale) for k, v in cw_dict.items()}\n",
    "    print(\"Scaled class weights:\", cw_dict)\n",
    "\n",
    "# Initialize sample weights = class weights\n",
    "sample_weight = np.array([cw_dict[int(lbl)] for lbl in y_train_arr], dtype=np.float32)\n",
    "\n",
    "# XGBoost baseline parameters (memory-optimized)\n",
    "xgb_params = dict(\n",
    "    objective='multi:softprob',\n",
    "    num_class=4,\n",
    "    tree_method='hist',\n",
    "    max_depth=6,\n",
    "    n_estimators=350,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.75,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2.0,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "model = None\n",
    "for it in range(n_rounds):\n",
    "    print(f\"\\n--- Iteration {it+1}/{n_rounds} — training with current sample weights ---\")\n",
    "    # Train model with current sample weights\n",
    "    model = XGBClassifier(**xgb_params)\n",
    "    model.fit(X_train_arr, y_train_arr, sample_weight=sample_weight, verbose=False)\n",
    "\n",
    "    # Predict probabilities on training set\n",
    "    proba = model.predict_proba(X_train_arr)  # shape (n_train, n_classes)\n",
    "    # p_t: probability assigned to the true class for each sample\n",
    "    p_t = proba[np.arange(len(y_train_arr)), y_train_arr].astype(np.float32)\n",
    "\n",
    "    # Focal factor: (1 - p_t) ** gamma\n",
    "    focal_factor = np.power(1.0 - np.clip(p_t, 1e-12, 1.0), gamma).astype(np.float32)\n",
    "\n",
    "    # New sample weights = class_weight * focal_factor\n",
    "    new_sample_weight = np.array([cw_dict[int(lbl)] for lbl in y_train_arr], dtype=np.float32) * focal_factor\n",
    "\n",
    "    # Stabilize total weight scale: keep same sum as previous iteration\n",
    "    prev_sum = sample_weight.sum()\n",
    "    new_sum = new_sample_weight.sum()\n",
    "    if new_sum > 0:\n",
    "        new_sample_weight = new_sample_weight * (prev_sum / new_sum)\n",
    "\n",
    "    sample_weight = new_sample_weight\n",
    "\n",
    "    # Diagnostics: average p_t and focal factor per class (training)\n",
    "    avg_pt = {}\n",
    "    avg_factor = {}\n",
    "    for c in classes:\n",
    "        mask = (y_train_arr == c)\n",
    "        if mask.sum() == 0:\n",
    "            avg_pt[c] = np.nan\n",
    "            avg_factor[c] = np.nan\n",
    "        else:\n",
    "            avg_pt[c] = float(p_t[mask].mean())\n",
    "            avg_factor[c] = float(focal_factor[mask].mean())\n",
    "    print(\"Avg p_t by class (train):\", avg_pt)\n",
    "    print(\"Avg focal factor by class (train):\", avg_factor)\n",
    "    gc.collect()\n",
    "\n",
    "# Final trained model after reweighting\n",
    "final_model = model\n",
    "\n",
    "# --------------------------\n",
    "# Probability calibration\n",
    "# --------------------------\n",
    "# Use a small stratified subset of the *training* set for calibration to avoid heavy cost\n",
    "calib_size = min(max_calib_samples, int(len(X_train_arr) * 0.1))\n",
    "if calib_size < 2000:\n",
    "    calib_size = min(2000, len(X_train_arr))\n",
    "\n",
    "# stratified sample\n",
    "X_cal, _, y_cal, _ = train_test_split(\n",
    "    X_train_arr, y_train_arr, train_size=calib_size, stratify=y_train_arr, random_state=random_state\n",
    ")\n",
    "print(f\"Using {len(X_cal)} samples to calibrate probabilities (sigmoid).\")\n",
    "\n",
    "# Wrap the prefit model with CalibratedClassifierCV (cv='prefit') and fit on small calibration set\n",
    "calibrated = CalibratedClassifierCV(base_estimator=final_model, method='sigmoid', cv='prefit')\n",
    "calibrated.fit(X_cal, y_cal)\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate on test set\n",
    "# --------------------------\n",
    "y_pred_internal = calibrated.predict(X_test_arr)\n",
    "y_pred = y_pred_internal + 1           # map back to 1-4 externally\n",
    "y_true = y_test_arr + 1\n",
    "\n",
    "print(\"\\n===== XGBOOST (C3: class weights + focal reweighting + calibration) =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc8b30af-4445-4c24-8af9-f192952e97b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\calibration.py:330: FutureWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== XGBOOST (C3: class weights + focal reweighting + calibration) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.20      0.14      0.17     13473\n",
      "           2       0.84      0.94      0.89   1086776\n",
      "           3       0.66      0.40      0.50    259851\n",
      "           4       0.35      0.21      0.26     36946\n",
      "\n",
      "    accuracy                           0.81   1397046\n",
      "   macro avg       0.51      0.42      0.45   1397046\n",
      "weighted avg       0.79      0.81      0.79   1397046\n",
      "\n",
      "Accuracy: 0.8088638455712983\n"
     ]
    }
   ],
   "source": [
    "calibrated = CalibratedClassifierCV(estimator=final_model, method='sigmoid', cv='prefit')\n",
    "calibrated.fit(X_cal, y_cal)\n",
    "\n",
    "# --------------------------\n",
    "# Evaluate on test set\n",
    "# --------------------------\n",
    "y_pred_internal = calibrated.predict(X_test_arr)\n",
    "y_pred = y_pred_internal + 1\n",
    "y_true = y_test_arr + 1\n",
    "\n",
    "print(\"\\n===== XGBOOST (C3: class weights + focal reweighting + calibration) =====\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6cda8c-780e-4160-af3b-e3a6b7c897e3",
   "metadata": {},
   "source": [
    "HIERARCHICAL SEVEERITY PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee5fa5f5-a031-48f1-954b-0929eac6ae18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== STAGE 1: LOW vs HIGH =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.90   1100249\n",
      "           1       0.65      0.39      0.49    296797\n",
      "\n",
      "    accuracy                           0.83   1397046\n",
      "   macro avg       0.75      0.67      0.69   1397046\n",
      "weighted avg       0.81      0.83      0.81   1397046\n",
      "\n",
      "\n",
      "===== HIERARCHICAL SEVERITY CLASSIFIER =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00     13473\n",
      "           2       0.78      0.87      0.82   1086776\n",
      "           3       0.19      0.12      0.15    259851\n",
      "           4       0.03      0.00      0.01     36946\n",
      "\n",
      "    accuracy                           0.70   1397046\n",
      "   macro avg       0.25      0.25      0.24   1397046\n",
      "weighted avg       0.64      0.70      0.67   1397046\n",
      "\n",
      "Accuracy: 0.701827284140966\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# ======================================\n",
    "# STAGE 1: LOW (1-2) vs HIGH (3-4)\n",
    "# ======================================\n",
    "y_stage1 = np.where(y <= 1, 0, 1)  # 0=Low, 1=High\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    X, y_stage1, test_size=0.2, random_state=42, stratify=y_stage1\n",
    ")\n",
    "\n",
    "clf_stage1 = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    tree_method='hist',\n",
    "    max_depth=5,\n",
    "    n_estimators=250,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf_stage1.fit(X_train1, y_train1)\n",
    "pred_stage1 = clf_stage1.predict(X_test1)\n",
    "\n",
    "print(\"\\n===== STAGE 1: LOW vs HIGH =====\")\n",
    "print(classification_report(y_test1, pred_stage1))\n",
    "\n",
    "# ======================================\n",
    "# SPLIT TEST DATA BASED ON STAGE 1 OUTPUT\n",
    "# ======================================\n",
    "idx_low  = np.where(pred_stage1 == 0)[0]\n",
    "idx_high = np.where(pred_stage1 == 1)[0]\n",
    "\n",
    "X_test_low  = X_test1[idx_low]\n",
    "X_test_high = X_test1[idx_high]\n",
    "y_test_low  = y_test.iloc[idx_low]      # original severity 0–3\n",
    "y_test_high = y_test.iloc[idx_high] \n",
    "\n",
    "# ======================================\n",
    "# STAGE 2A — LOW MODEL: SEVERITY 1 vs 2\n",
    "# ======================================\n",
    "mask_low = (y <= 1)\n",
    "X_low = X[mask_low]\n",
    "y_low = y[mask_low]\n",
    "\n",
    "clf_low = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=2,\n",
    "    tree_method='hist',\n",
    "    max_depth=5,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf_low.fit(X_low, y_low)\n",
    "pred_low = clf_low.predict(X_test_low)\n",
    "\n",
    "# ======================================\n",
    "# STAGE 2B — HIGH MODEL: SEVERITY 3 vs 4\n",
    "# ======================================\n",
    "mask_high = (y >= 2)\n",
    "X_high = X[mask_high]\n",
    "y_high = y[mask_high] - 2  # convert 2→0, 3→1\n",
    "\n",
    "clf_high = XGBClassifier(\n",
    "    objective='multi:softmax',\n",
    "    num_class=2,\n",
    "    tree_method='hist',\n",
    "    max_depth=5,\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    reg_lambda=2,\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "clf_high.fit(X_high, y_high)\n",
    "pred_high = clf_high.predict(X_test_high) + 2  # map back\n",
    "\n",
    "# ======================================\n",
    "# MERGE FINAL PREDICTIONS\n",
    "# ======================================\n",
    "y_pred_final = np.zeros_like(y_test1)\n",
    "y_pred_final[idx_low]  = pred_low\n",
    "y_pred_final[idx_high] = pred_high\n",
    "\n",
    "y_pred_final += 1      # convert internal 0–3 → real 1–4\n",
    "y_true_final = y_test + 1\n",
    "\n",
    "print(\"\\n===== HIERARCHICAL SEVERITY CLASSIFIER =====\")\n",
    "print(classification_report(y_true_final, y_pred_final))\n",
    "print(\"Accuracy:\", accuracy_score(y_true_final, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a211d5f-4007-4acb-b661-2ef3fa4ac690",
   "metadata": {},
   "source": [
    "Ordinal XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17077b41-512e-46d4-8886-08700351d577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 5588182\n",
      "Test samples: 1397046\n",
      "Class counts:\n",
      " Severity_Internal\n",
      "1.0    4347102\n",
      "2.0    1039406\n",
      "3.0     147783\n",
      "0.0      53891\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class weights: {0.0: 25.92354010873801, 1.0: 0.32137398662373234, 2.0: 1.3440806576063635, 3.0: 9.453357287374056}\n",
      "\n",
      "Training threshold model for y > 0.0 ...\n",
      "\n",
      "Training threshold model for y > 1.0 ...\n",
      "\n",
      "Training threshold model for y > 2.0 ...\n",
      "\n",
      "Finished training all ordinal models.\n",
      "\n",
      "===== ORDINAL XGBOOST RESULTS =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.0000    0.0000    0.0000     13473\n",
      "         1.0     0.7587    0.0883    0.1582   1086776\n",
      "         2.0     0.0415    0.1091    0.0602    259851\n",
      "         3.0     0.0057    0.0557    0.0104     36946\n",
      "         4.0     0.0000    0.0000    0.0000         0\n",
      "\n",
      "    accuracy                         0.0905   1397046\n",
      "   macro avg     0.1612    0.0506    0.0457   1397046\n",
      "weighted avg     0.5981    0.0905    0.1345   1397046\n",
      "\n",
      "Accuracy: 0.09046015664480625\n",
      "\n",
      "Predicted severity distribution:\n",
      " 1    126475\n",
      "2    682663\n",
      "3    360536\n",
      "4    227372\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\fokunlola\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# =====================================================\n",
    "# 1. LOAD DATA (ADAPT THIS PART IF NECESSARY)\n",
    "# =====================================================\n",
    "# Ensure df already preprocessed and Severity ∈ {1,2,3,4}\n",
    "\n",
    "X = df.drop(columns=['Severity_Internal'])\n",
    "y = df['Severity_Internal']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(X_train))\n",
    "print(\"Test samples:\", len(X_test))\n",
    "print(\"Class counts:\\n\", y_train.value_counts())\n",
    "\n",
    "# =====================================================\n",
    "# 2. COMPUTE CLASS WEIGHTS\n",
    "# =====================================================\n",
    "class_counts = y_train.value_counts().sort_index()\n",
    "total = len(y_train)\n",
    "K = len(class_counts)\n",
    "\n",
    "class_weights = {cls: total / (K * count) for cls, count in class_counts.items()}\n",
    "\n",
    "print(\"\\nClass weights:\", class_weights)\n",
    "\n",
    "sample_weight_train = y_train.map(class_weights)\n",
    "\n",
    "# =====================================================\n",
    "# 3. TRAIN ORDINAL XGBOOST MODELS\n",
    "# =====================================================\n",
    "ordinal_models = []\n",
    "classes = sorted(class_weights.keys())  # [1,2,3,4]\n",
    "\n",
    "for k in range(K - 1):  # train models for thresholds 1|2, 2|3, 3|4\n",
    "    print(f\"\\nTraining threshold model for y > {classes[k]} ...\")\n",
    "\n",
    "    y_binary = (y_train > classes[k]).astype(int)\n",
    "\n",
    "    dtrain = xgb.DMatrix(\n",
    "        X_train,\n",
    "        label=y_binary,\n",
    "        weight=sample_weight_train\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"eta\": 0.05,\n",
    "        \"max_depth\": 8,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"min_child_weight\": 3,\n",
    "        \"tree_method\": \"hist\"  # faster for large data\n",
    "    }\n",
    "\n",
    "    model = xgb.train(params, dtrain, num_boost_round=500)\n",
    "    ordinal_models.append(model)\n",
    "\n",
    "print(\"\\nFinished training all ordinal models.\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. PREDICT ON TEST SET\n",
    "# =====================================================\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "\n",
    "# P(y > k)\n",
    "P = np.column_stack([m.predict(dtest) for m in ordinal_models])\n",
    "\n",
    "probs = np.zeros((len(P), K))\n",
    "probs[:, 0] = 1 - P[:, 0]  # P(y = 1)\n",
    "\n",
    "for k in range(1, K - 1):\n",
    "    probs[:, k] = P[:, k - 1] - P[:, k]  # P(y = 2,3)\n",
    "\n",
    "probs[:, K - 1] = P[:, K - 2]  # P(y = 4)\n",
    "\n",
    "y_pred = np.argmax(probs, axis=1) + 1\n",
    "\n",
    "# =====================================================\n",
    "# 5. EVALUATE MODEL\n",
    "# =====================================================\n",
    "print(\"\\n===== ORDINAL XGBOOST RESULTS =====\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Optional: Show predicted distribution\n",
    "print(\"\\nPredicted severity distribution:\\n\", pd.Series(y_pred).value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa1299d4-8184-4b19-9c1c-773012826956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Distance(mi)', 'Temperature(F)', 'Visibility(mi)', 'Humidity(%)',\n",
       "       'Pressure(in)', 'Wind_Speed(mph)', 'Traffic_Signal', 'Junction',\n",
       "       'Crossing', 'Stop', 'State', 'City', 'Hour', 'Month', 'Weekday',\n",
       "       'Weekend', 'Night', 'Is_Rush_Hour', 'Rain', 'Snow', 'Fog',\n",
       "       'Low_Visibility', 'High_Wind', 'Temp_Below_Freezing', 'City_Frequency',\n",
       "       'Highway', 'Short_Distance', 'Long_Distance', 'Severity_Internal'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e710be-c082-4271-8c9f-c5fa706be7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
